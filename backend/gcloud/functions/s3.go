package functions

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"net/http"
	"strings"
	"time"

	"github.com/aws/aws-sdk-go-v2/aws"
	"github.com/aws/aws-sdk-go-v2/config"
	"github.com/aws/aws-sdk-go-v2/service/s3"
)

type Response struct {
	URL string `json:"url"`
}

func GetSignedURL(w http.ResponseWriter, r *http.Request) {
	// 1. CORS Headers
	w.Header().Set("Access-Control-Allow-Origin", "*")
	w.Header().Set("Access-Control-Allow-Methods", "GET, OPTIONS")
	w.Header().Set("Access-Control-Allow-Headers", "Content-Type")

	if r.Method == "OPTIONS" {
		return
	}

	ctx := context.TODO()

	// 2. Load Config with absolute error checking
	cfg, err := config.LoadDefaultConfig(ctx,
		config.WithRegion("us-east-1"), // AWS SDK requires a region even for custom endpoints
	)
	if err != nil {
		http.Error(w, "AWS Config Error: "+err.Error(), 500)
		return
	}

	// 3. Initialize Client safely
	s3Client := s3.NewFromConfig(cfg)
	presignClient := s3.NewPresignClient(s3Client)

	// 4. Explorer ID extraction and validation
	explorerID := getExplorerID(r)
	if explorerID == "" {
		http.Error(w, "explorer_id is required", 400)
		return
	}

	// 5. Determine upload path: "to" (companion -> cole), "from" (cole -> companion), or "staging" (temporary)
	// Default to "from" for Cole's responses, "to" for companion uploads
	path := r.URL.Query().Get("path")
	if path != "to" && path != "from" && path != "staging" {
		path = "from" // Default to Cole's responses
	}

	// 6. Check if this is an event bundle upload (new structure)
	eventID := r.URL.Query().Get("event_id")
	filename := r.URL.Query().Get("filename") // "image.jpg" or "metadata.json"

	var s3Key string
	if eventID != "" && filename != "" {
		if path == "staging" {
			// Staging is directly under bucket root: staging/{event_id}/{filename}
			s3Key = fmt.Sprintf("staging/%s/%s", eventID, filename)
		} else {
			// Event bundle structure: {explorerID}/{path}/{event_id}/{filename}
			s3Key = fmt.Sprintf("%s/%s/%s/%s", explorerID, path, eventID, filename)
		}
	} else {
		if path == "staging" {
			// Legacy staging structure: staging/{timestamp}.jpg
			s3Key = fmt.Sprintf("staging/%d.jpg", time.Now().Unix())
		} else {
			// Legacy single photo structure: {explorerID}/{path}/{timestamp}.jpg
			s3Key = fmt.Sprintf("%s/%s/%d.jpg", explorerID, path, time.Now().Unix())
		}
	}

	// 6. Presign the request
	// Check if method is GET (for downloads) or PUT (for uploads, default)
	method := r.URL.Query().Get("method")
	if method == "" {
		method = "PUT" // Default to PUT for backward compatibility
	}

	var presignedURL string

	if method == "GET" {
		// Generate GET presigned URL for downloading/viewing (Expiry: 4 hours)
		presignedRes, err := presignClient.PresignGetObject(ctx, &s3.GetObjectInput{
			Bucket: aws.String("reflections-1200b-storage"),
			Key:    aws.String(s3Key),
		}, s3.WithPresignExpires(4*time.Hour))
		if err != nil {
			http.Error(w, "Presign Error: "+err.Error(), 500)
			return
		}
		presignedURL = presignedRes.URL
	} else {
		// Generate PUT presigned URL for uploading (default)
		presignedRes, err := presignClient.PresignPutObject(ctx, &s3.PutObjectInput{
			Bucket: aws.String("reflections-1200b-storage"),
			Key:    aws.String(s3Key),
		})
		if err != nil {
			http.Error(w, "Presign Error: "+err.Error(), 500)
			return
		}
		presignedURL = presignedRes.URL
	}

	// 7. Successful JSON response
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]string{
		"url": presignedURL,
	})
}

// UploadToS3 is a helper for other functions in this package to upload data directly
func UploadToS3(ctx context.Context, key string, data []byte, contentType string) error {
	if len(data) == 0 {
		return fmt.Errorf("refusing to upload empty data to S3 at %s", key)
	}
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion("us-east-1"))
	if err != nil {
		return fmt.Errorf("failed to load AWS config: %w", err)
	}

	s3Client := s3.NewFromConfig(cfg)
	_, err = s3Client.PutObject(ctx, &s3.PutObjectInput{
		Bucket:      aws.String("reflections-1200b-storage"),
		Key:         aws.String(key),
		Body:        bytes.NewReader(data),
		ContentType: aws.String(contentType),
	})
	return err
}

// S3FileExists checks if a specific file exists in the event folder
func S3FileExists(ctx context.Context, s3Client *s3.Client, bucket, key string) bool {
	_, err := s3Client.HeadObject(ctx, &s3.HeadObjectInput{
		Bucket: aws.String(bucket),
		Key:    aws.String(key),
	})
	return err == nil
}

// EventMetadata represents the structure of metadata.json
// Note: audio_url is NOT stored here (presigned URLs expire after 15 min)
// The Event struct contains the fresh presigned GET URL generated by ListMirrorEvents
type EventMetadata struct {
	Description string `json:"description"`
	DeepDive    string `json:"deep_dive,omitempty"`
	Sender      string `json:"sender"`
	Timestamp   string `json:"timestamp"`
	EventID     string `json:"event_id"`
	ContentType string `json:"content_type,omitempty"` // "text" or "audio"
}

// Event represents a complete event bundle
type Event struct {
	EventID          string         `json:"event_id"`
	ImageURL         string         `json:"image_url"` // Always a thumbnail for video events
	MetadataURL      string         `json:"metadata_url"`
	AudioURL         string         `json:"audio_url,omitempty"` // Optional audio file URL
	VideoURL         string         `json:"video_url,omitempty"` // Optional video file URL
	DeepDiveAudioURL string         `json:"deep_dive_audio_url,omitempty"`
	Metadata         *EventMetadata `json:"metadata,omitempty"`
}

// ListMirrorEvents lists event bundles in Cole's inbox and returns presigned GET URLs
// for image.jpg, metadata.json, and audio.m4a (if present) for each event.
// Returns: JSON with "events" array, each containing event_id, image_url, metadata_url, and optional audio_url
func ListMirrorEvents(w http.ResponseWriter, r *http.Request) {
	// 1. Standard CORS
	w.Header().Set("Access-Control-Allow-Origin", "*")
	w.Header().Set("Access-Control-Allow-Methods", "GET, OPTIONS")
	w.Header().Set("Access-Control-Allow-Headers", "Content-Type")
	if r.Method == "OPTIONS" {
		return
	}

	ctx := context.TODO()

	// 2. Load Config with region
	cfg, err := config.LoadDefaultConfig(ctx,
		config.WithRegion("us-east-1"),
	)
	if err != nil {
		http.Error(w, "AWS Config Error: "+err.Error(), 500)
		return
	}

	// 3. Initialize Client and Presign Client
	s3Client := s3.NewFromConfig(cfg)
	presignClient := s3.NewPresignClient(s3Client)

	// 4. Explorer ID extraction and validation
	explorerID := getExplorerID(r)
	if explorerID == "" {
		http.Error(w, "explorer_id is required", 400)
		return
	}

	// 5. List objects in the "{explorerID}/to/" prefix (Cole's inbox)
	// Don't use delimiter - we need to see all nested objects
	input := &s3.ListObjectsV2Input{
		Bucket: aws.String("reflections-1200b-storage"),
		Prefix: aws.String(fmt.Sprintf("%s/to/", explorerID)),
	}

	result, err := s3Client.ListObjectsV2(ctx, input)
	if err != nil {
		http.Error(w, "S3 List Error: "+err.Error(), 500)
		return
	}

	// 6. Organize events by folder (event_id)
	eventMap := make(map[string]*Event)
	folderPrefix := fmt.Sprintf("%s/to/", explorerID)

	// Process all objects to find image.jpg and metadata.json files
	// (No need to process CommonPrefixes since we removed the delimiter)
	for _, obj := range result.Contents {
		key := *obj.Key
		// Skip the folder itself
		if key == folderPrefix {
			continue
		}

		// Extract event_id and filename from path like \"cole/to/{event_id}/image.jpg\"
		relativePath := key[len(folderPrefix):]
		parts := strings.Split(relativePath, "/")

		// Should have exactly 2 parts: [event_id, filename]
		if len(parts) == 2 {
			eventID := parts[0]
			filename := parts[1]

			// Ensure event exists in map
			if _, exists := eventMap[eventID]; !exists {
				eventMap[eventID] = &Event{
					EventID: eventID,
				}
			}

			// Generate presigned GET URL (Expiry: 4 hours)
			presignedRes, err := presignClient.PresignGetObject(ctx, &s3.GetObjectInput{
				Bucket: aws.String("reflections-1200b-storage"),
				Key:    aws.String(key),
			}, s3.WithPresignExpires(4*time.Hour))
			if err != nil {
				fmt.Printf("Error presigning %s: %v\n", key, err)
				continue
			}

			// Assign URL based on filename
			if filename == "image.jpg" {
				eventMap[eventID].ImageURL = presignedRes.URL
				fmt.Printf("Found image for event %s\n", eventID)
			} else if filename == "metadata.json" {
				eventMap[eventID].MetadataURL = presignedRes.URL
				fmt.Printf("Found metadata for event %s\n", eventID)
			} else if filename == "audio.m4a" || filename == "audio.mp3" || filename == "audio_caption.mp3" || filename == "caption.mp3" {
				eventMap[eventID].AudioURL = presignedRes.URL
				fmt.Printf("Found audio for event %s\n", eventID)
			} else if filename == "deep_dive.m4a" || filename == "deep_dive.mp3" || filename == "deep_dive_audio.mp3" {
				eventMap[eventID].DeepDiveAudioURL = presignedRes.URL
				fmt.Printf("Found deep dive audio for event %s\n", eventID)
			} else if filename == "video.mp4" {
				eventMap[eventID].VideoURL = presignedRes.URL
				fmt.Printf("Found video for event %s\n", eventID)
			}
		} else {
			// Log unexpected path structure for debugging
			fmt.Printf("Unexpected path structure: %s (parts: %v)\n", key, parts)
		}
	}

	// 6. Convert map to slice and fetch metadata for each event
	var events []Event
	for _, event := range eventMap {
		// Fetch metadata if URL exists
		if event.MetadataURL != "" {
			// Extract the S3 key from the presigned URL to fetch metadata
			// For now, we'll return the metadata URL and let the frontend fetch it
			// This keeps the response size manageable
		}
		events = append(events, *event)
	}

	// 7. Return as JSON
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"events": events,
	})
}

// GetEventBundle returns fresh presigned URLs for a specific event bundle
func GetEventBundle(w http.ResponseWriter, r *http.Request) {
	// 1. CORS Headers
	w.Header().Set("Access-Control-Allow-Origin", "*")
	w.Header().Set("Access-Control-Allow-Methods", "GET, OPTIONS")
	w.Header().Set("Access-Control-Allow-Headers", "Content-Type")

	if r.Method == "OPTIONS" {
		return
	}

	eventID := r.URL.Query().Get("event_id")
	if eventID == "" {
		http.Error(w, "event_id is required", 400)
		return
	}

	ctx := context.TODO()

	// 2. Load Config
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion("us-east-1"))
	if err != nil {
		http.Error(w, "AWS Config Error: "+err.Error(), 500)
		return
	}

	s3Client := s3.NewFromConfig(cfg)
	presignClient := s3.NewPresignClient(s3Client)

	// 3. Explorer ID extraction and validation
	explorerID := getExplorerID(r)
	if explorerID == "" {
		http.Error(w, "explorer_id is required", 400)
		return
	}

	// 4. List objects for this specific event folder: {explorerID}/to/{eventID}/
	prefix := fmt.Sprintf("%s/to/%s/", explorerID, eventID)

	input := &s3.ListObjectsV2Input{
		Bucket: aws.String("reflections-1200b-storage"),
		Prefix: aws.String(prefix),
	}

	result, err := s3Client.ListObjectsV2(ctx, input)
	if err != nil {
		http.Error(w, "S3 List Error: "+err.Error(), 500)
		return
	}

	event := &Event{EventID: eventID}

	// 4. Presign each file found in the bundle (Expiry: 4 hours)
	for _, obj := range result.Contents {
		key := *obj.Key
		filename := key[len(prefix):]

		presignedRes, err := presignClient.PresignGetObject(ctx, &s3.GetObjectInput{
			Bucket: aws.String("reflections-1200b-storage"),
			Key:    aws.String(key),
		}, s3.WithPresignExpires(4*time.Hour))

		if err != nil {
			fmt.Printf("Error presigning %s: %v\n", key, err)
			continue
		}

		switch filename {
		case "image.jpg":
			event.ImageURL = presignedRes.URL
		case "metadata.json":
			event.MetadataURL = presignedRes.URL
		case "audio.m4a", "audio.mp3", "audio_caption.mp3", "caption.mp3":
			event.AudioURL = presignedRes.URL
		case "deep_dive.m4a", "deep_dive.mp3", "deep_dive_audio.mp3":
			event.DeepDiveAudioURL = presignedRes.URL
		case "video.mp4":
			event.VideoURL = presignedRes.URL
		}
	}

	// Return JSON
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(event)
}

// DeleteMirrorEvent handles deletion of an event bundle (S3 objects)
func DeleteMirrorEvent(w http.ResponseWriter, r *http.Request) {
	// 1. Standard CORS
	w.Header().Set("Access-Control-Allow-Origin", "*")
	w.Header().Set("Access-Control-Allow-Methods", "DELETE, OPTIONS")
	w.Header().Set("Access-Control-Allow-Headers", "Content-Type")
	if r.Method == "OPTIONS" {
		return
	}

	ctx := context.TODO()

	// 2. Get event_id from query parameter (optional when extra_keys is provided for TTS-only cleanup)
	eventID := r.URL.Query().Get("event_id")
	extraKeysParam := r.URL.Query().Get("extra_keys")
	if eventID == "" && extraKeysParam == "" {
		http.Error(w, "event_id or extra_keys parameter required", 400)
		return
	}

	// 3. Explorer ID extraction and validation
	explorerID := getExplorerID(r)
	if explorerID == "" {
		http.Error(w, "explorer_id is required", 400)
		return
	}

	// 4. Load Config with region
	cfg, err := config.LoadDefaultConfig(ctx,
		config.WithRegion("us-east-1"),
	)
	if err != nil {
		http.Error(w, "AWS Config Error: "+err.Error(), 500)
		return
	}

	// 5. Initialize S3 Client
	s3Client := s3.NewFromConfig(cfg)

	// 6. Determine path: "to" (companion -> cole), "from" (cole -> companion, selfie responses), or "staging" (temporary)
	path := r.URL.Query().Get("path")
	if path != "to" && path != "from" && path != "staging" {
		path = "to" // Default to "to" for backward compatibility
	}

	bucket := "reflections-1200b-storage"

	var objectsToDelete []string
	if eventID != "" {
		if path == "from" {
			// For selfie responses: image.jpg and image_original.jpg (backup from shrink-images)
			objectsToDelete = []string{
				fmt.Sprintf("%s/%s/%s/image.jpg", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/image_original.jpg", explorerID, path, eventID),
			}
		} else if path == "staging" {
			// For staging, delete image.jpg (TTS files deleted via extra_keys)
			objectsToDelete = []string{
				fmt.Sprintf("staging/%s/image.jpg", eventID),
			}
		} else {
			// For regular reflections: all media + backups (image_original from shrink-images, video_original from shrink-videos)
			objectsToDelete = []string{
				fmt.Sprintf("%s/%s/%s/image.jpg", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/image_original.jpg", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/metadata.json", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/audio.m4a", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/deep_dive.m4a", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/video.mp4", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/video_original.mp4", explorerID, path, eventID),
				fmt.Sprintf("%s/%s/%s/video.mov", explorerID, path, eventID),
			}
			fmt.Printf("Attempting to delete objects for event %s: %v\n", eventID, objectsToDelete)
		}
	}

	// 6b. Append any extra_keys from query (for staging TTS cleanup: staging/{explorerID}/tts/*.mp3)
	if extraKeysParam != "" {
		var extraKeys []string
		if err := json.Unmarshal([]byte(extraKeysParam), &extraKeys); err == nil {
			for _, k := range extraKeys {
				if k != "" {
					objectsToDelete = append(objectsToDelete, k)
				}
			}
			fmt.Printf("Appended %d extra keys for deletion\n", len(extraKeys))
		}
	}

	var errors []string
	for _, key := range objectsToDelete {
		_, err := s3Client.DeleteObject(ctx, &s3.DeleteObjectInput{
			Bucket: aws.String(bucket),
			Key:    aws.String(key),
		})
		if err != nil {
			errors = append(errors, fmt.Sprintf("Failed to delete %s: %v", key, err))
			fmt.Printf("Error deleting %s: %v\n", key, err)
		} else {
			fmt.Printf("Successfully deleted %s\n", key)
		}
	}

	// 6. Return response
	w.Header().Set("Content-Type", "application/json")
	if len(errors) > 0 {
		w.WriteHeader(500)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"success": false,
			"errors":  errors,
		})
	} else {
		json.NewEncoder(w).Encode(map[string]interface{}{
			"success": true,
			"message": "Event deleted successfully",
		})
	}
}

type BatchUploadRequest struct {
	ExplorerID string   `json:"explorer_id"`
	EventID    string   `json:"event_id"`
	Path       string   `json:"path"`
	Files      []string `json:"files"`
}

// GetBatchS3UploadURLs generates presigned PUT URLs for multiple files in a single request
func GetBatchS3UploadURLs(w http.ResponseWriter, r *http.Request) {
	// 1. CORS Headers
	w.Header().Set("Access-Control-Allow-Origin", "*")
	w.Header().Set("Access-Control-Allow-Methods", "POST, OPTIONS")
	w.Header().Set("Access-Control-Allow-Headers", "Content-Type")

	if r.Method == "OPTIONS" {
		return
	}

	if r.Method != "POST" {
		http.Error(w, "Method not allowed", 405)
		return
	}

	// 2. Parse Request Body
	var req BatchUploadRequest
	if err := json.NewDecoder(r.Body).Decode(&req); err != nil {
		http.Error(w, "Invalid JSON: "+err.Error(), 400)
		return
	}

	if req.ExplorerID == "" {
		http.Error(w, "explorer_id required", 400)
		return
	}

	if req.EventID == "" {
		http.Error(w, "event_id required", 400)
		return
	}

	// 3. Load Config
	ctx := context.TODO()
	cfg, err := config.LoadDefaultConfig(ctx, config.WithRegion("us-east-1"))
	if err != nil {
		http.Error(w, "AWS Config Error: "+err.Error(), 500)
		return
	}

	s3Client := s3.NewFromConfig(cfg)
	presignClient := s3.NewPresignClient(s3Client)

	// 4. Default path logic
	path := req.Path
	if path != "to" && path != "from" && path != "staging" {
		path = "from"
	}

	// 5. Generate URLs
	urls := make(map[string]string)

	for _, filename := range req.Files {
		var s3Key string
		if path == "staging" {
			s3Key = fmt.Sprintf("staging/%s/%s", req.EventID, filename)
		} else {
			s3Key = fmt.Sprintf("%s/%s/%s/%s", req.ExplorerID, path, req.EventID, filename)
		}

		presignedRes, err := presignClient.PresignPutObject(ctx, &s3.PutObjectInput{
			Bucket: aws.String("reflections-1200b-storage"),
			Key:    aws.String(s3Key),
		})
		if err != nil {
			http.Error(w, "Presign Error for "+filename+": "+err.Error(), 500)
			return
		}
		urls[filename] = presignedRes.URL
	}

	// 6. Return Response
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"urls": urls,
	})
}
